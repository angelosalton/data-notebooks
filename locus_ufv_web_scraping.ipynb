{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "locus-ufv-web-scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqJe5AGULB8h",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/angelosalton/data-notebooks/blob/master/web-scraping-pyspark-analysis.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps4gMK9WKvqa",
        "colab_type": "text"
      },
      "source": [
        "# A tarefa\n",
        "\n",
        "Esse código tem como objetivo obter dados sobre trabalhos acadêmicos no [Locus UFV](https://www.locus.ufv.br/), o repositório de dissertações e teses da Universidade Federal de Viçosa, usando [Selenium](https://www.selenium.dev/about/), uma biblioteca para *web scraping*, ou raspagem de dados. Eu uso o Chrome como o navegador a ser automatizado, nesse caso é preciso configurar o `chromedriver` adequadamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5kPWncYK0C0",
        "colab_type": "text"
      },
      "source": [
        "# Configuração"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUDxAz5GK1x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "!pip install selenium\n",
        "#!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if8MbBRUKvqY",
        "colab_type": "text"
      },
      "source": [
        "# Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqdmupesKvqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "driver.get('https://www.locus.ufv.br/handle/123456789/2/recent-submissions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yl-OH-TKvqs",
        "colab_type": "text"
      },
      "source": [
        "O código a seguir obtém o título, autor e uma pequena parte do abstract/resumo. A partir daí, é possível fazer análises de texto e *clustering*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3NVtBNxKvqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store entries\n",
        "entries = []\n",
        "\n",
        "err = False\n",
        "while err == False:\n",
        "    # list of entries per page\n",
        "    elems = driver.find_element_by_css_selector('.ds-artifact-list')\n",
        "\n",
        "    # there are 20 entries per page\n",
        "    for i in range(20):\n",
        "        entries.append(elems.find_elements_by_css_selector('li.ds-artifact-item:nth-child({0})'.format(i+1))[0].text)\n",
        "    \n",
        "    # navigate to next page until there are no more pages\n",
        "    try:\n",
        "        nextpage = driver.find_element_by_css_selector('div.pagination:nth-child(1) > ul:nth-child(2) > li:nth-child(2) > a:nth-child(1)')\n",
        "        nextpage.click()\n",
        "    except NoSuchElementException:\n",
        "        err = True\n",
        "        print('Job complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWTZNf5A4olM",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, salvando o resultado em um `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGH0mTMnKvqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(list(map(lambda x: x.split('\\n'), entries)), columns=['title','author','abstract'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}